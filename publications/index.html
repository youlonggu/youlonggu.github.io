<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Youlong Gu </title> <meta name="author" content="Youlong Gu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://youlonggu.github.io/publications/"> <script src="/assets/js/theme.js?af57af4184f99a1de67fe636e52392ba"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Youlong</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/SPECS.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SPECS.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="quintana_global_2025" class="col-sm-8"> <div class="title">Global urban visual perception varies across demographics and personalities</div> <div class="author"> Matias Quintana, <span class="author-me">Youlong Gu</span>, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, and Filip Biljecki </div> <div class="periodical"> <em>Nature Cities</em>, Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s44284-025-00330-x" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Global%20urban%20visual%20perception%20varies%20across%20demographics%20and%20personalities.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Understanding people’s preferences is crucial for urban planning, yet current approaches often combine responses from multi-cultural populations, obscuring demographic differences and risking amplifying biases. We conducted a large-scale urban visual perception survey of streetscapes worldwide using street view imagery, examining how demographics—including gender, age, income, education, race and ethnicity, and personality traits—shape perceptions among 1,000 participants with balanced demographics from five countries and 45 nationalities. This dataset, Street Perception Evaluation Considering Socioeconomics, reveals demographic- and personality-based differences across six traditional indicators—safe, lively, wealthy, beautiful, boring, depressing—and four new ones: live nearby, walk, cycle, green. Location-based sentiments further shape these preferences. Machine-learning models trained on existing global datasets tend to overestimate positive indicators and underestimate negative ones compared to human responses, underscoring the need for local context. Our study aspires to rectify the myopic treatment of street perception, which rarely considers demographics or personality traits.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">quintana_global_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global urban visual perception varies across demographics and personalities}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{2025 The Author(s), under exclusive licence to Springer Nature America, Inc.}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2731-9997}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nature.com/articles/s44284-025-00330-x}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s44284-025-00330-x}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Cities}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Nature Publishing Group}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Quintana, Matias and Gu, Youlong and Liang, Xiucheng and Hou, Yujun and Ito, Koichi and Zhu, Yihan and Abdelrahman, Mahmoud and Biljecki, Filip}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Cultural and media studies, Geography, Psychology and behaviour}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1092--1106}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/CUPUM.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CUPUM.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gu_bayesian_2025" class="col-sm-8"> <div class="title">A Bayesian Spatiotemporal Framework for Explaining Bus Ridership Dynamics in Singapore</div> <div class="author"> <span class="author-me">Youlong Gu</span>, Haixiao Liu, Lingsheng Lan, Yu He, and Filip Biljecki </div> <div class="periodical"> <em>The 19th International Conference on Computational Urban Planning and Urban Management (CUPUM)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.17605/OSF.IO/ABYQH" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/A%20Bayesian%20Spatiotemporal%20Framework%20for%20Explaining%20Bus%20Ridership%20Dynamics%20in%20Singapore.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Bus networks serve as the backbone of urban public transport, significantly shaping daily mobility patterns. However, few studies have leveraged long-term high spatiotemporal resolution bus data to precisely analyze service demand and urban-scale ridership dynamics. This study proposes a Bayesian hierarchical spatiotemporal modelling framework that integrates structured and unstructured spatial and temporal components. Using Singapore —a city with a dense bus network — as a case study, we evaluate the framework’s effectiveness, identify spatiotemporal trends and ridership hotspots, and assess the influence of key spatial covariables. The findings offer valuable insights for urban and transport planning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gu_bayesian_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A {Bayesian} {Spatiotemporal} {Framework} for {Explaining} {Bus} {Ridership} {Dynamics} in {Singapore}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://osf.io/abyqh/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.17605/OSF.IO/ABYQH}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 19th International Conference on Computational Urban Planning and Urban Management (CUPUM)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OSF}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Youlong and Liu, Haixiao and Lan, Lingsheng and He, Yu and Biljecki, Filip}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{agent-based modelling, artificial intelligence, cloud cities, data ownership, digital infrastructure, digital twin, land use, machine learning, public data, spatial analysis, sustainability, transportation, urban planning, virtual systems}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/LLM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="LLM.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cai_can_2025" class="col-sm-8"> <div class="title">Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels</div> <div class="author"> Chenyi Cai, Kosuke Kuriyama, <span class="author-me">Youlong Gu</span>, Filip Biljecki, and Pieter Herthogs </div> <div class="periodical"> <em>ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5194/isprs-annals-X-4-W7-2025-1-2025" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Can%20a%20Large%20Language%20Model%20Assess%20Urban%20Design%20Quality%20Evaluating%20Walkability%20Metrics%20Across%20Expertise%20Levels.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Urban street environments are vital to supporting human activity in public spaces. The emergence of big data, such as street view images (SVI) combined with multi-modal large language models (MLLM), is transforming how researchers and practitioners investigate, measure, and evaluate semantic and visual elements of urban environments. Considering the low threshold for creating automated evaluative workflows using MLLM, it is crucial to explore both the risks and opportunities associated with these probabilistic models. In particular, the extent to which the integration of expert knowledge can influence the performance of MLLM in the evaluation of the quality of urban design has not been fully explored. This study set out an initial exploration of how integrating more formal and structured representations of expert urban design knowledge (e.g., formal quantifiers and descriptions from existing methods) into the input prompts of an MLLM (ChatGPT-4) can enhance the model’s capability and reliability to evaluate the walkability of built environments using SVIs. We collect walkability metrics through the existing literature and categorise them using relevant ontologies. Then we select a subset of these metrics, used for assessing the subthemes of pedestrian safety and attractiveness, and develop prompts for MLLMs accordingly. We analyse MLLM’s abilities to evaluate SVI walkability subthemes through prompts with multiple levels of clarity and specificity about evaluation criteria. Our experiments demonstrate that MLLMs are capable of providing assessments and interpretations based on general knowledge and can support the automation of imagetext multimodal evaluations. However, they generally provide more optimistic scores and can make mistakes when interpreting the provided metrics, resulting in incorrect evaluations. By integrating expert knowledge, MLLM’s evaluative performance exhibits higher consistency and concentration. Therefore, this paper highlights the importance of formally and effectively integrating domain knowledge into MLLMs for evaluating urban design quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cai_can_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can a {Large} {Language} {Model} {Assess} {Urban} {Design} {Quality}? {Evaluating} {Walkability} {Metrics} {Across} {Expertise} {Levels}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{X-4-W7-2025}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2194-9042}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://isprs-annals.copernicus.org/articles/X-4-W7-2025/1/2025/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5194/isprs-annals-X-4-W7-2025-1-2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Copernicus GmbH}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cai, Chenyi and Kuriyama, Kosuke and Gu, Youlong and Biljecki, Filip and Herthogs, Pieter}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{ontology, public space evaluation, semantic and visual elements, street view images, walkability assessment}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gu_designing_2025" class="col-sm-8"> <div class="title">Designing effective image-based surveys for urban visual perception</div> <div class="author"> <span class="author-me">Youlong Gu</span>, Matias Quintana, Xiucheng Liang, Koichi Ito, Winston Yap, and Filip Biljecki </div> <div class="periodical"> <em>Landscape and Urban Planning</em>, Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.landurbplan.2025.105368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Designing%20effective%20image-based%20surveys%20for%20urban%20visual%20perception.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Urban visual perception is important for the human experience in cities, shaped by intertwined characteristics of urban landscapes. By quantifying and explaining these perceptual experiences, researchers can gain insights into human preferences and support decision-making in planning and design. However, past studies have shown inconsistencies in survey design and ambiguities in reporting, leading to concerns about the reliability and reproducibility of results. This study proposes the first comprehensive framework to guide image-based survey design for capturing perceptions of outdoor urban environments across different scenarios, addressing the lack of methodological standardization in current research. We reviewed existing surveys to identify key parameters, conducted comprehensive between-subject and within-subject surveys, and performed statistical analyses to determine best practices for survey design across different contexts. Aiming to set a potential community standard, our study doubles as a blueprint for a reporting protocol for survey designs. Based on the results, we recommend: (1) meeting a minimum of 12 and 22 ratings per image for Likert Scale and Pairwise Comparison studies to reach survey reliability, respectively, and reporting these alongside other survey design parameters to enhance transparency and reproducibility; and (2) when resource allows larger experiments, adopt a ranking method such as Pairwise Comparison to achieve firmer rating results; and (3) using perspective (non-panoramic) images more frequently, as they exhibit comparable overall scores to panoramic images (R mostly \textgreater0.7), while being more widely available via crowdsourced sources, supporting their use in large-scale visual perception research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gu_designing_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Designing effective image-based surveys for urban visual perception}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{260}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0169-2046}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0169204625000751}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.landurbplan.2025.105368}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2026-01-17}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Landscape and Urban Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Youlong and Quintana, Matias and Liang, Xiucheng and Ito, Koichi and Yap, Winston and Biljecki, Filip}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Built environment, Street view imagery, Urban perception, Human participants, Survey parameters}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105368}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/ACM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ACM.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="quintana_my_2024" class="col-sm-8"> <div class="title">My street is better than your street: Towards data-driven urban planning with visual perception</div> <div class="author"> Matias Quintana, <span class="author-me">Youlong Gu</span>, and Filip Biljecki </div> <div class="periodical"> <em>In Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3671127.3698700" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">quintana_my_2024</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Hangzhou China}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{My street is better than your street: {Towards} data-driven urban planning with visual perception}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-8-4007-0706-3}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dl.acm.org/doi/10.1145/3671127.3698700}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3671127.3698700}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 11th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Quintana, Matias and Gu, Youlong and Biljecki, Filip}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{221--222}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Youlong Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 19, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>